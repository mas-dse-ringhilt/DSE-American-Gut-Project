{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an example of using `redbiom` through it's Python API to extract a subset of American Gut Project samples. These data are then loaded into QIIME 2 for a mini beta-diversity analysis using UniFrac. This assumes we're using a QIIME 2 2018.11 environment that additionally has `redbiom` 0.3.0 installed. The exact commands I ran to install it are:\n",
    "\n",
    "```\n",
    "$ conda install nltk\n",
    "$ pip install https://github.com/biocore/redbiom/archive/0.3.0.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redbiom.summarize\n",
    "import redbiom.search\n",
    "import redbiom.fetch\n",
    "import qiime2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we're going to do is gather the `redbiom` contexts. A context is roughly a set of consistent technical parameters. For example, the specific sequenced gene, the variable region within the gene, the length of the read, and how the operational taxonomic units were assessed.\n",
    "\n",
    "The reason `redbiom` partitions data into contexts is because these technical details can lead to massive technical bias. The intention is to facilitate comparing \"apples\" to \"apples\". \n",
    "\n",
    "The context summarization returns a pandas `DataFrame` so it should be pretty friendly to manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = redbiom.summarize.contexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the present time, determining the context to use is a bit manual and requires some strung munging. Additional development is needed.\n",
    "\n",
    "Let's take a look at the larger contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts.sort_values('SamplesWithData', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity sake, let's take the first context. It's large, and the phylogeny associated with the operational taxonomic units is easy to get. We'll break down the meaning of the context name in a moment. In practice, you will _most likely_ want to use the Deblur data, however producing a reasonable tree from those data requies a slightly computationally expensive step, and I'm on my laptop right now with limited battery quite literally in the middle of nowhere on a bus in the Czech Republic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = contexts.sort_values('SamplesWithData', ascending=False).iloc[0]['ContextName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking this name into its constiuent pieces, this is a closed reference context meaning that operational taxonomic units were assessed against a reference database and sequences which did not recruit to the reference were discarded. The reference used is Greengenes, a common 16S reference database. The gene represented by the data is the 16S SSU rRNA gene, and specifically the V4 region of the gene. Finally, the fragments represented are truncated to 100 nucleotides. (Don't worry if this is all a lot of jargon. It is a lot of jargon. Please ask questions :)\n",
    "\n",
    "So cool, we have a \"context\". What can we do now? Let's search for some sample identifiers based off of the metadata (i.e., variables) associated with the samples. Specifically, let's get some skin, oral and fecal samples. Be forewarned, the metadata search uses Python's `ast` module behind the scenes, so malformed queries at present produce tracebacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_id = 10317  # the Qiita study ID of the American Gut Project is 10317\n",
    "results = {}\n",
    "for site in ['sebum', 'saliva', 'feces']:\n",
    "    query = \"where qiita_study_id==%d and env_material=='%s'\" % (study_id, site)\n",
    "    results[site] = redbiom.search.metadata_full(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in results.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the sample sets are not very balanced. For the purposes of the mini-analysis, let's just grab the first 100 from each sample type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = []\n",
    "for k, v in results.items():\n",
    "    to_keep.extend(list(v)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last output cell shows what these IDs look like. These are Qiita sample IDs.\n",
    "\n",
    "Now that we have some samples, let's get some data! What we're going to do is ask `redbiom` to obtain the sample data, for our `to_keep` samples, from the context we previously selected. What's happening behind the scenes is that the API is pulling out sparse vectors corresponding to the number of individual sequences observed for each operational taxonomic unit per sample, and additionally unmunging the names (as `redbiom` normalizes sample and feature identifiers). The output is then aggregated into what's called a BIOM `Table`, which is really just a rich object wrapped around a `scipy.sparse` matrix. \n",
    "\n",
    "You may noice two outputs on the return. The one we're ignoring represents \"ambiguous\" samples. Some sample identifiers are associated with multiple sequenced samples. This is because some samples may \"fail\" sequencing, where they didn't yield sufficient sequence data, and were rerun. These \"failures\" are still represented in Qiita, but are differentiated by the actual sequencing run they were on. This doesn't matter for the moment though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biom_table, _ = redbiom.fetch.data_from_samples(ctx, to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biom_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `repr` output shows that we have roughly 13k OTUs (operational taxonomic units), and only 244 samples. What gives? We were supposed to get 300! Just because a sample has metadata does not mean it has sequence data. It is also possible that some of the samples haven't been run through the same bioinformatic processing (e.g., closed reference at 100nt).\n",
    "\n",
    "More information on `biom` can be found [here](http://biom-format.org/). \n",
    "\n",
    "Let's play with the object for just a moment for familiarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biom_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(biom_table.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the table, it implicitly casts it to a dense matrix representation. One thing to note: the sample IDs look different than before, right? The way that the individual sequencing runs are denoted is the last number -- the sample ID is represented by \"qiita_study_id.the_actual_id.the_sequencing_run_id\". This is to ensure the identifier is \"globally\" unique.\n",
    "\n",
    "The identifiers on the rows are the \"Greengenes\" IDs. Also called OTU IDs. Or Feature IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a table. Now let's get the sample metadata. This will come down as a pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md, _ = redbiom.fetch.sample_metadata(to_keep, context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.sample_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So we have a bunch of sample data, and some study variables! Now for the QIIMEing. What we'll need to do is pacakge these data into types that QIIME 2 understands. And in particular, we need to use the semantic type system. This is well documented on the QIIME 2 website, and I recommend reviewing there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ar = qiime2.Artifact.import_data('FeatureTable[Frequency]', biom_table)\n",
    "md_ar = qiime2.Metadata(md.set_index('#SampleID'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! We need a tree! Since we're using Greengenes, we can just rely on the existing prebuilt tree from the reference. Let's get that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget ftp://ftp.microbio.me/greengenes_release/gg_13_8_otus/trees/97_otus.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_ar = qiime2.Artifact.import_data('Phylogeny[Rooted]', '97_otus.tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. So next up, we need to rarefy these data. Rarefaction is a just randomly subsampling the samples without replacement in order to normalize for sequencing effort. There are a lot of ways to do this type of normalization. Rarefaction is dirty as you through out data, but it's pragmatic and tends to work pretty well. The issue rarefaction helps to ameliorate is that, the more you sequence the more life you'll observe. So if you don't do something to normalize sequencing effort across samples, you'll have a bad time.\n",
    "\n",
    "Let's import all the plugins we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiime2.plugins import feature_table, diversity, emperor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rarefy to 1000 sequences per sample (yes, it's arbitrary)\n",
    "rare_ar, = feature_table.actions.rarefy(table=table_ar, sampling_depth=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unweighted_unifrac_ar, = diversity.actions.beta_phylogenetic(table=rare_ar, \n",
    "                                                             phylogeny=tree_ar, \n",
    "                                                             metric='unweighted_unifrac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So before we move on, we should discuss what this voodoo just did. Beta diversity is what ecologists call comparing how similar (or dissimilar) two samples are. For example, how similar is a forest in the Pacific Northwest to a forest in Maine? (i.e., beta diversity is a function which takes two vectors and returns a scaler). We perform this calculation over all pairs of samples though so that we can examine all of the sample relationships. This distance matrix gets large quick: in our case, it's already 244 by 244. \n",
    "\n",
    "Visualizing that matrix would suck. So, one thing we often do is principal *coordinates* analysis. It's very similar to principal *components* analysis, except that we can pass in our distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unweighted_unifrac_pcoa_ar, = diversity.actions.pcoa(unweighted_unifrac_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's actually view the coordinates. It's much more interesting to use the metadata as well :) Hint: color by body site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz, = emperor.actions.plot(unweighted_unifrac_pcoa_ar, md_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiim2-2018.11",
   "language": "python",
   "name": "qiim2-2018.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
